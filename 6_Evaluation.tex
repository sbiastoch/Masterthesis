\chapter{Evaluation and discussion}\label{results_discussion}

This chapter evaluates the performance of the automatic generated translation. I compare runtimes of various versions of the same functions. I compare the original function against its translation. I measure the effect of the presented optimizations and compare the translation against a manually optimized formulation.

Plots are drawn using logarithmic x- and y-axis to depict the large differences in runtimes. Additionally, polynomial growth is better to differentiate from exponential when using logscales. A polynomial function on a xy-logscale appears as a straight line, the slope determines the degree of the polynomial. A linear function, ie. a polynomial of degree 1 has a slope of 1.

\paragraph*{Evaluation setup}
The evaluations were run on a System running Ubuntu 18.04.1 with 4.15 Linux-Kernel, AMD 1900X with 8x 3.80GHz (16 Threads), 256MB NVMe SSD and 16GB DDR4 RAM. PostgreSQL 10.6 with default settings, if not stated otherwise.

The call stack of PostgreSQL is limited in size, to prevent infinite recursion from quickly sucking up the entires system memory. The default in PostgreSQL 10.6 (\texttt{max\_stack\_depth}) is 2MB and can be manually increased. The maximum stack size is capped by the settings of the Operating System.\\
The available working memory (\texttt{work\_mem}) specifies how much memory can be consumed by in-memory operations like sorting or hashing. If the memory is exceeded, temporary results are materialized on disk. The default working memory is 2MB. \cite[p. 512 ff.]{psql}

To fully exploit the capabilities of each implementation, the \texttt{max\_stack\_size} is set to 7680kB (the limit of the OS) and working memory (\texttt{work\_mem}) to 512MB. This way recursive functions can run longer before being killed. The high working memory enables operations like sorting or hashing to be performed in memory, even for larger inputs.

The original, recursive functions were run until the stack size was exceeded and the function was killed. The translations were run (where possibly) at least as long as the original function until the trend of the curves seems to be clear.

To track plans and times of parts of the translation the extension \texttt{auto\_explain} was used with \texttt{log\_nested\_statements}, \texttt{log\_verbose} and \texttt{log\_timing} enabled. Logging format (\texttt{log\_format}) was set to JSON.

\section{Generic Template}

The generic template is the default template without any UDF-specific optimizations as presented in \autoref{template}. The performance of the Fibonacci function (\autoref{intro_fib}) from the introduction is shown in \autoref{fig:fib_times}.

\begin{figure}[h!]
    \centering\small
    \includesvg{plots/fib}
    \caption{Run time of the fibonacci function in its recursive version and the automatic translation. Additionally, a manually optimized version is compared.}
    \label{fig:fib_times}
\end{figure}

, the automatic translation and the third, manually optimized fibonacci function\footnote{From \url{https://wiki.postgresql.org/wiki/Fibonacci_Numbers}, accessed: 15/12/2018}. To represent the wide variety the plots are drawed with logarithmic scales.

Runtimes vary greatly between the three implementation. The original implementation is faster for very small inputs as it has less overhead then the translation. Additionally, the first two (\texttt{fib(\$n-1)} and \texttt{fib(\$n-2)}) calls are inlined, which reduces overhead further. Yet, the runtime of the original function grows very fast, so that \texttt{fib(30)} already takes minutes. Finally, the recursive UDF is killed by PostgreSQL at a input of 32???

The translated function on the other hand is much faster. Where the original takes minutes, the translation is finished under 10ms. Even for \texttt{fib(1000)} the call returns after around 2 seconds. For very small inputs the runtime seems to be nearly constant, possibly because the tiny computations are marginalized by the constant overhead of the template. After leveling up, the runtime of the translation has a slope of approximietly 2, indicating quadratically growth.

Yet, the translation cannot keep up with the manually optimized function. The step from the translation to the manually optimized version is similar in magnitude as the step from the original to the translation: The runtime drops from minutes to milliseconds.\\
But even here the runtime grows as the inputs gets even larger, but slower. The implementation provided runs nearly in linear time, exploiting that only two values needs to be remembered to compute the next value.\\

Eventually, other limitations finally stop computing even larger fibonacci numbers in PostgreSQL: Somewhere between fib(600000) and fib(700000) the numeric datatype (which can represent numbers up to 131072 digits! \cite[p. 124 f.]{psql}) fails with an overflow. That said, the manual optimized implementation is "as good as we can get in SQL".

Each implementation has changed the complexity-class of the implementation. The recursive implementation of \texttt{fib} is exponential \cite{} as previously computed values are repeatedly recomputed. Actually, the calltrees of the callsites are identical except a single call.

Problems with overlapping subproblems as with fibonacci are subject to an optimization technique called "Dynamic Programming" and "Memoization". Starting from optimal subproblems solutions of larger problems are incremental computed. Dynamic Programming requires you therefore to reformulate the algorithm to implement this approach. Memoization solves the problem top-down, keeping the original recursive structure of the function. For each call it is checked if this call already was evaluated. If so, the saved value is returned. Our translation uses the original recursive formulation and applies automatic memoization, avoiding any redundant calls.

If an algorithm has no overlapping subproblems and there are no redundant calls, memoization gives no speedup. Each calltree of the callsites is disjunct. The evaluation of this algorithms grounds on recursivly dividing the problem in disjunct subproblems. The results are than recursivly merged to construct the final result ("divide and conquer"). An example with two callsites is \texttt{quicksort} (\autoref{udfs:quicksort}).

\begin{figure}[h!]
    \centering\small
    \includesvg{plots/nonoverlapping}
    \caption{Runtimes for quicksort as a quadratic divide and conquer algorithm.}
    \label{fig:nonoverlapping_times}
\end{figure}

\autoref{fig:nonoverlapping_times} shows that the translation still causes a speedup ca. by factor 10 for \texttt{quicksort}. This is remarkable because the function does not profit from memoization. This plot illustrates boldly that PostgreSQL is highly optimized for efficient operations on tables and not for efficient function calls.

As last aspect of the generic translation, let us have a look at the fraction of time spent in each part of the translation \autoref{fig:split_times}. We see that the evaluation is by far the costliest part of the translation. Notably is the large fraction that the bare result selection takes. This can be explained, that each iteration of the evaluation works only on a linear growing working-table and appends its results without duplicate checking to the final result. The total result include all the lots of duplicates is only seen when selecting the result. Therefore both parts have constantly a significant influence of the total runtime, opposed to callgraph and basecase, that marginalize over time.

\begin{figure}[h!]
    \centering\small
    \includesvg{plots/split}
    \caption{Percentage of runtime for each part of the translation of \texttt{fib}}
    \label{fig:split_times}
\end{figure}


\iffalse
\begin{figure}[h]
    \centering\small
    \includesvg{plots/recN}
    \caption{Impact of the number of scenarios on the runtime. Single recursive function with expensive predicates}
    \label{fig:recn_times}
\end{figure}

Impact of number of scenarios (Overhead by predicates) \autoref{fig:recn_times}

\fi




\section{Single and Tail Recursion}

Functions with only one callsite per scenario are called single recursive. I count them in the context of this thesis to the group of divide and conquer algorithms, as they have no overlapping subproblems. I use the \texttt{sum} (\autoref{udfs:sum}) function as exmaple in the following to study the performance of the translation.

\begin{figure}[h!]
    \centering\small
    \includesvg{plots/sum}
    \caption{Sum as example for a single recursive function. Calls to the original \texttt{sum} eventually exceed stack depth.}
    \label{fig:factorial_times}
\end{figure}


Surprisingly, the generic translation of \texttt{sum} yields to a runtime even worse than the original quicksort function. Quicksort was run with an array of descending numbers, thus the runtime equals the worst case of $O(n^2)$. The original single recursive version of \texttt{sum} grows roughly linear as we can see from the plot. The generic translation of \texttt{sum} degrades the performance of the linear function heavily. It is even worse than quicksort (\autoref{fig:nonoverlapping_times}), which is quadratic!

\autoref{fig:split_sr_times} depicts the fraction of time each part of the translation consumes from the total computation time for a given input. Left, the data is taken from the badly performing, generic translation. Right, the translation using the single recursive optimization. We see, that in the generic translation evaluation marginalizes all other parts of the translation.

\begin{figure}[h!]
    \centering\small
    \includesvg{plots/split_sr}
    \caption{Fraction of total runtime spend in each part of the translation of \texttt{sum}.}
    \label{fig:split_sr_times}
\end{figure}

This slowdown is apparently caused by the growth of the evaluation CTE. All previous results are accumulated there and each result adds all previous results to allow access to results from all earlier iterations. This behaviour makes the evaluation-CTE grow quadratically, even if the original function is linear.

But this does not explain why the function performs worse than the quadratic, non-memoizable quicksort? The problem is caused by the wrong guess of the query planner about the number of rows returned from the scan of \texttt{callgraph}. During scenario-evaluation the CTE \texttt{e} (proxying \texttt{evaluation}) is joined with \texttt{callgraph} two times: Once for joining callsites with their results, here a hash join is used ($O(n+m)$, good). The other join is caused by the \texttt{NOT EXISTS} which is rewritten into a \texttt{LEFT JOIN}.

\begin{figure}[h!]
    \centering\scriptsize
    \begin{verbatim}
->  Nested Loop Anti Join  (cost=0.00..20.54 rows=1 width=68) (actual time=0.118..15.206 rows=250 loops=500)
	Output: c_3.in_1, c_3.call_site, c_3.out_1
	Join Filter: (e_1.in_1 = c_3.in_1)
	Rows Removed by Join Filter: 83333
	->  CTE Scan on callgraph c_3  (cost=0.00..4.29 rows=1 width=68) (actual time=0.000..0.077 rows=499 loops=500)
	    Output: c_3.in_1, c_3.call_site, c_3.out_1
	    Filter: (c_3.call_site = ANY ('{1}'::integer[]))
	    Rows Removed by Filter: 1
	->  CTE Scan on e e_1  (cost=0.00..10.00 rows=500 width=32) (actual time=0.000..0.011 rows=168 loops=249500)
	    Output: e_1.in_1, e_1.res
    \end{verbatim}
    \caption{Extract of the query plan of scenario evaluation from \texttt{sum(500)}, translated using the generic template.}
    \label{plan:sum}
\end{figure}

The planner assumes that only a single row is returned from \texttt{callgraph} after filtering with \texttt{c.callsite\_id IN (1)} and chooses therefore to make a \textit{nested loop anti join}. Nested loop joins are a good choice if one table is small, because the runtime is $O(nm)$ \cite{}. But the UDF has only a single callsite, therefore the predicate is not selective at all and not one but all $n$ rows are returned. The working-table of evaluation has on average a size of $n/2$. Therefore, the join complexity is in $O(n^2)$ - for a single iteration. 

Evaluation of the callgraph happens in a level-wise fashion, one level per iteration. Recalling \autoref{}, the callgraph of a single recursive function is just a linked list so the depth of the callgraph is $n$. For quicksort the callgraph is a binary tree, so the depth is just $log_2(n)$. In sum, the generic translation of a single recursive functions increases complexity from $O(n)$ to $O(n^3)$.

Using the template for single recursion fixes this issue. The template does not accumulate all previous results in the current result and thus the working-table does not grow with $n$ but yields always only a single row. The size of \texttt{evaluation} remains linear.

The original function starts out linear but slows down a bit for large inputs before exceeding stack size. The speedup by the optimized translation is roughly by factor 10, with no limitation by the maximum stack size.

The tail recursive optimization speeds up performance constantly by the factor of approx. 2. This seems natural considering that \autoref{fig:split_sr_times} shows that the evaluation-CTE, which is omitted in the tail recursive template, accounts for ca. 40\% of the runtime.

\section{Template Optimizations}


\begin{figure}[h!]
    \centering\small
    \includesvg{plots/paramN_opts}
    \caption{Impact of the different optimizations, compared to the original function \texttt{param4} (see \autoref{udfs:paramN})}
    \label{fig:paramN}
\end{figure}



\autoref{fig:paramN} looks surprising. The original functions perform all equally and much better than the translations. Even more interstingly, the UDF with just one argument performs worst. This behaviour is surprising as we would guess that a simpler functions performs better in the translation because there is less data to manage and jonining tables is easier.\\
A first idea is, that the additional arguments give the functions more selective join predicates. Yet, this cannot be the case as the additional arguments of the used functions are just the arguments passed to the function, so the values are equal for all calls.\\
The reason for the big difference lays probably in the join order of the evaluation-subqueries, as the query plan reveals \autoref{}. The function with one argument and n=200 makes first a nested loops anti join to join \texttt{evaluation} (rows=500, actual=68) with \texttt{callgraph} (rows=1, actual=201). The result (rows=1, actual=100) is joined with an hash join with \texttt{evaluation} (rows=500, actual=101).\\
The UDF with four arguments has flipped the order of the joins: First a hash join between \texttt{evaluation} (rows=940, actual=102) and \texttt{callgraph} (rows=100, actual=201) happens. The result (rows=1, actual=101) is joined with a nested loop anti-join with \texttt{evaluation} (rows=940, actual=69).

Nested Loop Joins are a good choice when one table is small, otherwise a hash join performs better. The other plan uses first a hash join and than the nested loop join. As result, the nested loop join is executed on a smaller table which explains the better run-time. The plan from the UDF with one argument performs a nested loop join on tables with 68 and 201 rows, while in the other plan the inputs are sized 69 and 101 rows.

Now to the observation that the original performs better than the translation. This can be explained by the property, that the given function is linear recursive. No branching occurs and no memoization can be exploited. The overhead of the translation is significant is such a case: The evaluation-CTE adds in each step all the previous results to the new result. The number of rows is therefore $(0 + 1) + (1 + 1) + (2 + 1) + \dots + (n + 1) = \frac{n(n+1)}{2}$ and thus in $O(n^2)$. With the translation, we have made a linear function quadratically. Note that the results were obtained with optimizations disabled. So there is hope, as we see in the next section comparing optimizations to their originals.


% When limiting stack size to 100kb, param1_tr(320) is the first argument where it dies with "stack depth limit exceeded". As the function is nearly as simple as it gets, is seems to be valid to assume as lower bound that SQL will consume 1/3 kB per layer on the callstack. With default OS settings of max of 8MB on my machine, this means a maximum recursion depth of not more than roughly 25000 layers.  


\begin{figure}
    \centering
    \include{tikz/fib_callstack_memoization.tex}
    \caption{Callgraph with memoization, The callstack-tree becomes a Directed Acyclic Graph (DAG). Each node represents an invocation and each outgoing edge a new callsite. Because of memoization, evaluation of equal invocations is performed only once, ie. they have the same descendants.}
    \label{fig:fib_callstack_memoization}
\end{figure}



\section{Future work}

This thesis provides only a first working version the general approach developed by the Database Systems Research Group at TÃ¼bingen and there are many topics for future work.

\subsection{Template optimization}
The template is already highly optimized but there is still room for improvements. There are some low-hangig fruits and some topics that need further research.

\paragraph*{Evaluation of predicate-parts in own CTE}
Currently, predicates are created during scenario analysis. Each predicate duplicates all the previous predicates by including them negated. If each part is evaluated once in a CTE and then just referenced in further predicates, this redundant evaluations can be eliminated. Queries with costly predicates and many scenarios would profit from this small change.\\
This could happen either in a post-processing step of the predicates or the predicate in the rules is replaced with an supporting data-structure.

\paragraph*{\texttt{LIMIT}s}
In the evaluation we have seen that wrong guesses have led the query planner to serious misconceptions, impacting runtime severly. By adding explicit \texttt{LIMIT}s to subqueries we give the planner explicit information about how many rows to expect. Furthermore, expensive joins can be stopped as soon as the first and only row has been returned. For example the single recursive template will always just return a single row per iteration of the evaluation-CTE. Therefore, scenario evaluation can be stopped after the single row is returned.\\
The same is applicable for final result collection: Instead of doing a \texttt{DISTINCT}, a \texttt{LIMIT 1} would have the same effect. Definitively there are further possibilities to add explicit \texttt{LIMIT <n>} statements, maybe from an analysis of the callgraph.

\paragraph*{Nullable arguments}
Currently, only UDFs can be translated with non-\texttt{NULL} arguments. Joining tables with \texttt{=} yields false when comparing \texttt{NULL}-values. Yet, \texttt{NULL} can be an valid input. \texttt{IS NOT DISTINCT FROM} does consider two \texttt{NULL}s as equal, but the join-performance is heavily impacted as experiments have shown. This is reasonable, since the join-predicate \texttt{x IS NOT DISTINCT FROM y} is just syntactic sugar for \texttt{x = y OR (x IS NULL AND y IS NULL)}.

The solution may be to distinguish between \texttt{STRICT} and \texttt{NON STRICT} UDFs. Functions tagged as \texttt{STRICT} are functions that return \texttt{NULL} straight away if any of their arguments is \texttt{NULL}. The actual function is not evaluated in this case. Therefore, no call with a \texttt{NULL} argument will lead to subsequent callsites, so that all recursive scenarios have non-\texttt{NULL} arguments. Calls with a \texttt{NULL}-argument are therefore only required to consider when collecting basecases. During evaluation all arguments are not nullable and \texttt{=} can be used safely. Argument-comparisons for \texttt{NON STRICT} UDFs would fallback to use \texttt{IS NOT DISTINCT FROM}.

\subsection{Extend applicability}

The current translation template does not work for every possible recursive UDF. It would be very nice if some of the constraints could be lifted.

\paragraph*{Allow row-variables in predicates and arguments}
This is a constraint that limits the real world usage of the translation the most. SQL is made for operations on tabular data but the translation barely support this yet. The challange is here to extract an evaluable fragment of code from an arbitrary position in an arbitrarily nested query. The fragment must be evaluated for every possible value of its row-reference. The callgraph gets so a new dimension: A scenario is evaluable if all callsites have results for every row available. Surely, there are a number of other complexities and challenges which must be solved first.

\paragraph*{Allow dependant callsites}
Another constraint is that callsites may not depend on each other. Recursive function arguments like $f(f(n-1))$ are not allowed as the argument must be evaluated in advance to create the callgraph. The Ackermann function is a prominent example for such a function.

Another form of dependency between callsites is of two recursive predicates at the same level. This is unfortunate, because any form of backtracking algorithm uses this pattern: If a recursive call with current arguments fails, backtrack recursively. The recursive call in the predicate would cause the scenario analysis to return the entire fragment as a single scenario.\\
This limitation may be lifted with an iterative partial construction of the callgraph.

\paragraph*{Translation from and to other languages}

The UDF-slices from the scenario analysis could be used to fill templates in another target language, eg. PL/SQL or C. The general idea from the translation could also be applied to other languages.

\subsection{Optimize evaluation}

The callgraph is a precious source of information about the runtime-behaviour of the recursive function. If we can exploit this information, evaluation can happen much more efficiently.

The problem with the workaround to allow back-references in self-referential CTEs is that there will always accumulate many, many redundant rows. These duplicates cannot be removed afterwards from the final result of the CTE and need to be traversed in order to select the result of the evaluation.

An other option is to use the callgraph to readd only these rows, that are required by later callsites. In each iteration of the evaluation, one topological level of the callgraph is evaluated. It is easy to check after each iteration which nodes in the callgraph have no more dependencies to callsites that are not already evaluated. Those values can be safely removed from the result set, as this results will not be needed again.

An other possibility is to analysis the maximum dependency distance to include only the last $n$ results in every iterations result. For the Fibonacci-function, such a analysis would limit the size of the working table constantly to 2. This approach could give a great speedup for many problems and would be easy to implement.

Furthermore, we could analyze the callgraph and annotate points of interest. It may be beneficial to prune the evaluation table periodically or at advantageous points, eg. when a large subtree is completely evaluated. Pruning could happen by restarting evaluation, keeping only required rows.

An other option is to persist tables in an unlogged temporary table. This has the advantage that rows could be easilly removed if not required anymore. This could happen as a periodical side-effect during translation. Further, tables could be indexed and different processes could parallely work together. Especially divide and conquere algorithms could benefit as they need only few evaluation passes and produce a large number of disjunct calls.\\
Persisted tables could also be used to memoize calls not only from the current query, but also from earlier queries. This could be used to implement a caching mechanism.

Analyzing the callgraph may reveal independant subtrees that could be evaluated in parallel. This could either happen by manually triggering parallel execution by starting own processes that access common tables or by utilizing PostgreSQL own parallel-query capabilities. 