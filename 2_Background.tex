\chapter{Background}\label{background}

\section{Relational Databases}\label{theory}

In this chapter I give a short overview on aspects of the area of relational database systems that are relevant to my thesis. I introduce the relational data model and its Structured Query Language (SQL). All explanations target PostgreSQL 10.6 as state-of-the-art Database Management System (DBMS).

\subsection{Relational Data Model and PostgreSQL}
An integral part of many applications is to retrieve, modify and persist data. Depending on the complexity of the data (eg. plain key-value, hierarchical structured, associations to other data, etc.) and the requirements to the storage system (performance, query capabilities, robustness, multi-user capability, etc.) a number of solutions are viable. Relational Database Management Systems (RDBMS) are a very popular and advanced solution to efficiently manage vast amounts of interconnected data.

The original relational data model dates back to 1970 when E. F. Codd proposed a representation of data in set-theoretic \textit{relations} \cite{codd}. A relation is similar to a table with named and typed columns. An instance of a relation is a set of tuples from the \textit{domain} of each of the columns. A subset of columns in a relation may form a \textit{primary key} uniquely identifying each of its tuples. The primary key can be included in other tables as \textit{foreign key} to establish references between rows. A relational data model should be normalized to remove redundancies and enforce atomacity.

An important innovation by the relational model was the claim that access to the stored data should be separated from its internal (physical) representation. The representation should be able to change without affecting the logical representation that is used by a client (\textit{data independence}). Instead of manually accessing the physical stored data, the client should just use a "data base sublanguage" (Codd) to specify tasks on the logical level, and the RDBMS automatically would find an efficient way to perform the task on a physical level \cite[p. 3 ff.]{FoD}.

Based on Codds proposal of a "universal data sublanguage" to define and manipulate a relational model, Chamberlin and Boyce designed a "Structured English Query Language" (SEQUEL) \cite{sequel, sequel2}. The name alone highlights its declarative nature: Say \textit{what} you want, just in plain English. SEQUEL was later renamed to SQL and for the first time standardized in 1987 as SQL:86 (ISO 9075:1987). The latest version today is SQL:2016 (ISO/IEC 9075:2016). For this thesis, only a subset of the \textit{Data Manipulation Language} (DML) is of interest, namely reading queries.

PostgreSQL 10.6 is used throughout this thesis as RDBMS and SQL-dialect. Its first version was developed under the name of "POSTGRES" at Berkely in 1986 \cite[xxxvi ff.]{psql} and released as open source called "Postgres95" in 1995, where its own query language PostQUEL was replaced by SQL. As of 1997 and version 6.0, the software is named "PostgreSQL" to reflect its SQL-capabilities \cite[xxxvi ff.]{psql}.

PostgreSQL claims to be "the most advanced open-source database available anywhere" \cite[xxxvii]{psql}. It conforms to most parts of Core SQL:2011 \cite[S. 2198]{psql} and offers great extensibility and introspection. User defined functions, types, operators, aggregates, index methods, statistics for the planner, procedural languages, foreign data wrappers and rewrite rules are possible.

\subsection{SQL and query evaluation}

Due to their declarative nature, SQL-Queries are no executable programs by themselves. In order to create a program from the query (a \textit{plan}), it initially has to be parsed into an \textit{Abstract Syntax Tree} (AST, \autoref{fig:fib_ast}). An AST encodes the syntactic structure of the query into the structure of the AST, preserving its semantics.

Parsing text into an AST is a standard step in compilation because ASTs are much more suitable to work with \cite[p. 5]{dragenbook}. My implementation uses two great libraries that were internally developed at WSI TÃ¼bingen to receive and pretty-print a sanitized AST from a SQL-query \cite{denis_hirn, peter_richter}.

\begin{figure}[h!]
    \centering\footnotesize
    \begin{tikzpicture}[level distance=1cm,level 1/.style={sibling distance=2cm},level 2/.style={sibling distance=1cm}, every node/.style = {shape=circle, draw, align=center, minimum width=7mm, minimum height=7mm}]]
    \node {\texttt{+}}
        child { node{\texttt{fib}}
            child { node {\texttt{-}}
                child { node {n} }
                child { node {1} }
            }
        }
        child { node{\texttt{fib}}
            child { node {\texttt{-}}
                child { node {n} }
                child { node {2} }
            }
        };
    \end{tikzpicture}
    \caption{AST of \texttt{fib(n-1) + fib(n-2)}}
    \label{fig:fib_ast}
\end{figure}

The \textit{parse-tree} of the input query is transformed into a \textit{query-tree}. The transformation to the query-tree includes some desugaring like expanding \texttt{T.*} to an explicit column-list, adding default \texttt{ELSE NULL} to \texttt{CASE}-statements and expanding row-comparisons \texttt{(1, 2) = (1, 4)} to boolean conjunctions \texttt{1=1 AND 1=4}. This makes it easier for the query planner to analyze the structure of the query and derive an efficient plan. 

In the end, the query is rewritten, which primary replaces references to views by their original queries. \cite[p. 1072 ff.]{psql}

The preprocessed and rewritten query is then fed into the \textit{query planner} to compile the declarative query into an procedural program (a \textit{plan-tree}) executable by the \textit{executor}. Its main responsibility is to find the fastest way to access and join the involved tables. Every (nontrivial) plan has table scans at its leafs. There are multiple ways to scan a table depending on the presence of predicates and auxiliary data-structures (\textit{indices}).

Consider the query \texttt{SELECT * FROM T WHERE T.v = 5}. Depending on the size of \texttt{T} and the number of occurrences of \texttt{5} in the column \texttt{T.v} it may be most efficient to do a full \textit{sequential scan} of the entire table, filtering out rows not matching the predicate. If the table is large and there are only few rows with \texttt{T.v = 5}, an \textit{index-scan} may be much faster. Using the index, the matching rows can be selectively retrieved without reading the entire table. Various statistics are maintained to help the planner estimate the cost for each possible access path and choose the cheapest plan. \cite[p. 1887]{psql}


Tables in the \texttt{FROM}-clause can be combined (\textit{joined}) by different algorithms. Because each algorithm has its own preconditions, strengths and weaknesses,  the choice of the appropriate join-strategy is very important. As each join-algorithm expects exactly two input tables, joining multiple tables results in a binary join-tree. The join order can impact the query performance dramatically, since it determines the size of the intermediate join-results. Three tables $R$, $S$, $T$ can either be joined as $(R \bowtie S) \bowtie T$ or as $R \bowtie (S \bowtie T)$. In general, it is beneficial to choose the one option which removes most rows from the result early on to avoid investing time in joining rows that are eventually removed anyways. The planner tries to attach predicates from the query to the joins to reduce the size of the join result (\textit{predicate push-down}).

Further nodes in the plan are used for sorting, aggregation, combining queries and others. Obviously there are a lot of possible plans for a single query. To find the best plan, the query planner computes all possible plans and estimates the cost of each solution based on various statistics to select the cheapest. As the search-space for the planner growths exponentially with the number of involved tables, the exhaustive search switches to a heuristic-driven optimization when a threshold is exceeded \cite[p. 2064 ff.]{psql}.


Eventually, the \textit{executor} recursively executes each node of the plan just as long as required by the parent node (Volcano Iterator Model) \cite{volcano}. Thus each operation is not necessarily performed to completion but just as long as required to return the next row. Therefore each plan contains not only an estimate of the \textit{total cost} of the node until completion, but also of the minimum cost to return the first row (\textit{start up cost}). In the best case, the first returned row is just the single requested row and accordingly the entire remaining table scan can be omitted. Yet, some operations need to be run to completion before being able to return the first row, eg. sorting. For these cases, the start up cost equals the actual cost.

% Building the cartesian product between tables is called a \textit{join}. Even for moderate sized tables the result of a couple of joins can be astronomically large, if done naively. Imagine build the cartesian product a table \texttt{S} with 10.000 rows, \texttt{T} with 100 rows and \texttt{R} with 1.000 rows. The resulting table would have $10.000 * 100 * 1.000 = 1.000.000.000$ rows - that escalated quickly!

% They just specify \textit{what} the user wants to achieve without telling \textit{how} (declarativity \cite[p. 21]{ullman}). To understand the "problem statement" of the user, the query is parsed, checked and rewritten before fed into the query-planner.

% It is possible to output the generated parse- and query-trees to the logfiles with configuration parameters \texttt{debug\_print\_\{parse, rewritten\}} set to true. To sanitize and parse the query-tree again, I used the great library \texttt{PgQueryHauler} by Denis Hirn \cite{denis_hirn} and Peter Richter \cite{peter_richter} to parse the logs and pretty print them as SQL again.

% There may be many possible strategies to execute a query and the best solution depends on various details (physical organization, frequencies of values, auxiliary data-structures, etc. pp) that may help to find the most efficient one. The \textit{query planner} is responsible to generate an actual executable, imperative program called a \textit{plan}.

%They specify \textit{what} the user wants to achieve without requiring any information about the exact \textit{how}. A query to count the number of rows with \texttt{t=0} from a table \textit{T} does state this intent directly as \texttt{SELECT COUNT(*) FROM T WHERE t = 0}. There is no loop incrementing a counter nor any information included about the location or retrieval of the contents of \texttt{T}. The desired result is specified at a high abstraction level and the best way to compute this result is obtained automatically. This property is known as \textit{declarativity}.

% To evaluate the query, the "problem statement" (the query) needs to be translated into a concrete "plan". This translation is done by the query planner.

% But even the query planner is not the holy grail and there are queries that perform incredibly bad or are difficult to formulate in a way the query planner can find a efficient plan.

% The most frequent form of reading queries are $\textit{SELECT s_1, s_2, ..., s_n FROM T_1, T_2, ..., T_n WHERE p}$-queries (S-F-W). The cartesian product $\texttt{T_1 \times T_2 \times \dots \times T_n}$ between the tables is built and the resulting rows are filtered by the predicate \texttt{p}. Finally, the output columns in the select-list are evaluated for each row, returning the output table.
\section{Recursion in SQL}

Recursion is a fundamental concept in everyday programming as well as in computability-theory. Every recursive function consists of a number of different \textit{evaluation scenarios}. Some of them cause subsequent recursive calls (\textit{recursive scenarios}) and at least one scenario causes no recursive calls at all (\textit{nonrecursive} or \texttt{basecase scenario}). Otherwise, the function would never terminate.

The programmer usually uses conditionals to direct which scenario should be evaluated. For this thesis I assume conditionals to be given in the form of \texttt{CASE}-expressions only, so that a \textit{predicate} is stated explicitly for each scenario. One of the main tasks of the translation will be to collect the scenarios and identify their predicates.

Recursive functions can be implemented in SQL in two ways: As a User Defined Function (UDF) or as a self-referential Common Table Expression (CTE) in \texttt{WITH RECURSIVE}-queries. UDFs are functions (actually procedures) as we know them: A function with a name, an argument list, a defined return type and a function body. Recursive CTEs are different as they are no procedures with explicit arguments but self-referential queries of a specific structure.

\subsection{Recursive User Defined Functions}
Recursive functions can be created in PostgreSQL as UDFs. UDFs can be implemented in a wide range of languages but we consider only SQL-UDFs as we do not want to leave the SQL-world. PostgreSQL provides a number of different ways to specify arguments and add modifiers. For my thesis I assume just simple positional arguments without any modifiers (eg. \texttt{\$1}).

\begin{figure}[h!]
    \centering
    \begin{cminted}{postgresql}
CREATE FUNCTION fac(INT) RETURNS INT AS $$
SELECT CASE WHEN $1 = 1 THEN 1
            ELSE $1 * fac($1 - 1)
       END
$$ LANGUAGE SQL;

SELECT fac(10);
    \end{cminted}
    \caption{Calculation of $10!$ using a recursive UDF \texttt{fac}.}
    \label{udf:fac}
\end{figure}

UDFs are mostly a black box for the query planner. They have just a fixed cost assigned as a whole (default 1000 \cite[p. 1435]{psql}) and the body of every call is planned before execution.

Under certain circumstances, PostgreSQL can optimize some function calls. Very simple functions can be inlined during planning \cite{psqlWikiUDFinlining} (see \autoref{fig:inlining}), but this is done only once thus its impact for recursive functions is not a game-changer.


\begin{figure}[h!]
    \centering
    \begin{minted}{postgresql}
CASE WHEN ($1 = 1) THEN 1
     ELSE ($1 * CASE WHEN (($1 - 1) = 1) THEN 1
                     ELSE (($1 - 1) * fac(($1 - 1) - 1))
                END)
END
    \end{minted}
    \caption{The query-body of \texttt{fac} after the planner has inlined the call to \texttt{fac(\$n - 1)}.}
    \label{fig:inlining}
\end{figure}

Identical calls to a function marked as \texttt{IMMUTABLE} can be folded into a constant during planning \cite[p. 995]{psql}. The impact on average queries may be significant when a call to an expensive function (eg. in a where-clause) is repeated for every row of a table:\\
\begin{minted}{postgresql}
SELECT * FROM T WHERE T.v IS BETWEEN fac(99) AND fac(100);
\end{minted}
Instead of evaluating \texttt{fac(99)} and \texttt{fac(100)} exactly $|T|$ times the planner may be able to evaluate those functions once and replace the values in the where-clause with the values obtained before. For recursive SQL-UDFs this happens as well, but isolated for each invocation of an UDF. There is nothing folded into constants across levels of recursion.

\subsection{Recursive Common Table Expressions}

Common Table Expressions (CTEs) are a convenient way to define auxiliary queries once in advance to use them similar to temporary tables in the contained query. CTEs help breaking a big query down into smaller individual parts. CTEs are only evaluated once which can improve performance. On the other hand, CTEs are known to be "optimization fences" as the planner does not perform predicate pushdown into CTEs.


\begin{figure}[h!]
    \centering
    \begin{minted}{postgresql}
WITH RECURSIVE T(i, v) AS (
 SELECT 1, 1
   UNION ALL
 SELECT i + 1, v * (i + 1) FROM T WHERE i < 10
)
SELECT v FROM T WHERE i = 10;
    \end{minted}
    \caption{Calculation of $10!$ using a self-referential CTE.}
    \label{fig:my_label}
\end{figure}


CTEs also exist in a "recursive" variant which makes SQL queries much more expressive. Actually, since the introduction of recursive CTEs in SQL:1999, queries are turing-complete. Every intuitively computable function can be expressed as a SQL-query. PostgreSQL supports this feature since Version 8.4 (2009) \cite[p. 2811]{psql}. Implementations in PostgreSQL for turing-machines and cyclic tag systems exist demonstrating this property \cite{psqlWikiCTS, psqlWikiTM}.

However, recursive functions need to be implemented in a special style using self-referential CTEs inside \texttt{WITH RECURSIVE}-queries. This constraints origin by the fact that they are actually evaluated iteratively. Each self-referential CTE consist of a \texttt{UNION} or \texttt{UNION ALL} dividing the starting query from the "recursive" query. The recursive query contains a reference to the CTE itself. Evaluation happens in a iterative fashion: In each iteration the self-reference points to a \textit{working table} containing the results from the previous iteration. When using the variant with \texttt{UNION} each iteration returns only (globally) new rows. The rows of each iteration are added to the result of the CTE.

As soon as the working table is empty the evaluation stops. In the case of \texttt{UNION ALL} this happens only when an iteration returns the empty table. Using \texttt{UNION} it stops as soon as the iteration returns no previously unseen rows. Self-referential CTEs therefore resemble a while loop.

The semantics of a self-referential CTE using \texttt{UNION} can be summarized as follows:

$$
q_0 ~\cup ~\underbrace{q_r(q_0)}_{q_1} ~\cup~ \underbrace{q_r(q_1)}_{q_2}~ \cup~\underbrace{q_r(q_2)}_{q_3}~ \cup ~\hdots ~ \cup ~ \underbrace{q_r(q_n)}_{= \emptyset}
$$

For \texttt{UNION ALL} the set semantics is replaced with bag semantics to allow duplicates.

Besides the special structure of a self-referential CTE it comes with two important restrictions. First, only a single (direct) self-reference is allowed in the self-referential query. A simple workaround is to use a CTE that "proxies" access thus this limitation is not severe.

More notably, only the previous iteration can be referenced. This is a strong limitation as this allows only the convenient formulation of linear recursive functions. In linear recursion a single recursive call leads to a single subsequent call. To evaluate a call only the result of the previous call is therefore required. The usual workaround is to use \texttt{UNION ALL} to allow duplicates and to add the previous result to the new results. The obvious drawback from this solution is the resulting table becoming large quickly. After $n$ iterations the results from the first iteration are included $n$ times, results from the second iteration $n-1$ times and so on. The result table grows quadratically. Furthermore, one has to take care to include a stopping criterion to eventually prevent the inclusion of the previous rows again and instead return the empty table.



\section{Related Work}
%The trade-off between readability and performance

The transformation of inefficient recursive algorithms into more time/space efficient iterative versions has been subject of research for a long time and different solutions have been developed \cite{redundantRecRemoval, transformingRecProgs, recursionRemoval, changeEvalOrder, programTransformation}.

A widely known approach is Dynamic Programming (DP) \cite{DP_Bellman}. DP is an implementation technique to optimize problems with overlapping subproblems by avoiding redundant calls. It is applicable if a problem has the "optimal subproblem" property: An optimal solution contains only optimal sub-solutions. The algorithm is then formulated in a way to construct bottom-up optimal solutions first and extend these solutions successively. Optimal solutions to subproblems are usually stored in a table-like data-structure. \cite[p. 363 ff.]{Cormen}

Incrementalization is an approach to systematically derive a DP-version from a recursively stated algorithm. The idea is to find the minimal increment by inverting the minimal decrement found in the function. The difficulty is to find the inverse of the increment-function which generally requires the use of some "eureka-function", ie. manual assistance. This makes it hard to fully automate this approach. \cite{incrementalization}

Closely related to DP is Memoization \cite{memo} which we also apply in our approach. While DP requires to state the problem in a bottom-up, "DP-way" fashion, Memoization can be implemented less invasive. The top-down recursive structure remains unchanged, only the recursive function is replaced with a "memo" function. The memo-function saves each value it has computed. Before a recursive call is made the program checks whether there is a saved value for a call with these arguments. For pure functions like in Lisp this can even happen at a language-level \cite{norvig_memoization}. \cite[p. 390 ff.]{Cormen}

Performing computations close to the data within a RDBMS is favorable especially in situations where the data does not fit into main memory. In particular, computations for statistical and machine learning applications can profit from the scalability of RDBMSs \cite{SQLforML,PCAinSQL,MLwithUDFs,MLinSQL2,decisionTables}.

Recursion in SQL has been studied mostly on a language level. Different approaches exist to improve the performance or capabilities of self-referential queries \cite{LRoptimizations, optimizingRecursiveQueries, extendingRecursionInSQL, RCTEplanningOptimization}. Recursive UDFs (in other RDBMS known as \textit{Stored Procedures}) seem to be a sparsely studied topic. This may be due to implementations of recursive SQL only becoming widely available in the last couple of years \cite{RSQLsurvey} and recursive UDFs being no explicit part of the SQL standard.